Line #    Mem usage    Increment   Line Contents
================================================
    28   25.129 MiB    0.000 MiB   @profile
    29                             def constructGraph(dset):
    30   25.133 MiB    0.004 MiB       time1 = time.time()
    31   25.133 MiB    0.000 MiB       with warnings.catch_warnings(): # giati ta scipy.sparse vgazoun DeprecationWarning
    32   25.133 MiB    0.000 MiB           warnings.simplefilter("ignore")
    33   25.133 MiB    0.000 MiB           with open(dset, 'rb') as tempFile: # diavazei to arxeio
    34   25.133 MiB    0.000 MiB               reader = csv.reader(tempFile)
    35   25.133 MiB    0.000 MiB               q = []
    36   25.133 MiB    0.000 MiB               l = []
    37   27.273 MiB    2.141 MiB               for row in reader:
    38   27.273 MiB    0.000 MiB                   if (row[0] != '') and (row[1] != ''): # xwrizei tis dyo sthles tou arxeiou se dyo listes
    39   27.273 MiB    0.000 MiB                       q.append(row[0])
    40   27.273 MiB    0.000 MiB                       l.append(row[1])
    41   27.270 MiB   -0.004 MiB           time2 = time.time()
    42   27.273 MiB    0.004 MiB           print( 'Read Time: %s seconds' %(time2 - time1) )
    43                                     # Lists with unique values:
    44   27.273 MiB    0.000 MiB           uq=uniqueValues(q) # dhmiourgei listes me monadika stoixeia
    45   27.344 MiB    0.070 MiB           ul=uniqueValues(l)
    46   27.344 MiB    0.000 MiB           time3 = time.time()
    47   27.344 MiB    0.000 MiB           print( 'Find Unique Values: %s seconds' %(time3 - time2) )
    48                                     # n=number of unique queries, p=number of unique url:
    49   27.344 MiB    0.000 MiB           n=len(uq)
    50   27.344 MiB    0.000 MiB           p=len(ul)
    51                                     # Map nodes to queries/url:
    52                                     global Q 
    53   29.867 MiB    2.523 MiB           Q = mapper(uq) # dhmiourgei ordered dictionaries pou antistoixizoun queries me nodes
    54                                     global L
    55   32.238 MiB    2.371 MiB           L = mapper(ul)
    56   32.238 MiB    0.000 MiB           del uq, ul # ypotithetai oti ta eleytherwnei ap th mnhmh, den kserw ti paizei me to garbage collector
    57   32.238 MiB    0.000 MiB           time4 = time.time()
    58   32.238 MiB    0.000 MiB           print( 'Mapping Time: %s seconds' %(time4 - time3) )
    59                                     # Accumulative graph:
    60   33.047 MiB    0.809 MiB           E = lil_matrix((n,p)) # Arxikopoiei ton pinaka E. Ftiaxnoume lil_matrix giati einai pio grhgoroi sthn dhmiourgia.
    61   33.934 MiB    0.887 MiB           for i in xrange(len(q)):
    62   33.934 MiB    0.000 MiB               E[Q.get(q[i])-1,L.get(l[i])-1] +=1 # Dhmioyrgeitai pinakas me antitoixies query-url pou metraei ton arithmo tous. Oi grammes antistoixoun se queries oi sthles se url kai oi times sto plithos twn forwn poy clickare kapoios url me vash to sygkekrimeno query.
    63   33.934 MiB    0.000 MiB           del q, l
    64   33.934 MiB    0.000 MiB           time5 = time.time()
    65   33.934 MiB    0.000 MiB           print( 'Construct Accumulative Matrix: %s seconds' %(time5 - time4) )
    66   34.230 MiB    0.297 MiB           sumQ = np.asarray(E.sum(axis = 1)) # sumQ einai to athroisma twn grammwn. deixnei to degree tou kathe query node
    67   35.750 MiB    1.520 MiB           sumL = np.asarray(E.sum(axis = 0)) # sumL einai to athroisma twn stilwn. deixnei to degree tou kathe url node
    68   35.750 MiB    0.000 MiB           time6 = time.time()
    69   35.750 MiB    0.000 MiB           print( 'Find Node Degrees Time: %s seconds' %(time6 - time5) )
    70   35.750 MiB    0.000 MiB           print( 'Total Time: %s seconds' %(time6 - time1) )
    71   37.297 MiB    1.547 MiB           G = lil_matrix((n+p,n+p)) # Adjacency matrix. Einai tetragwnos pinakas megethous (n+p)x(n+p) opou oi prwtes n grammes/sthles einai ta queries kai oi epomenes p grammes/sthles einai ta url. Logw tou grafou pou exoume o panw aristera ypopinakas nxn kai o katw deksia apo n+1 ews n+p, einai adeioi (afou den syndeontai q me q kai l me l ston grafo mas)
    72   38.102 MiB    0.805 MiB           T = lil_matrix((n,p)) # Einai o katw aristera ypopinakas tou G anastramenos
    73   38.102 MiB    0.000 MiB           print( 'Query Weight Association:' )
    74   36.859 MiB   -1.242 MiB           G = G.todense() # kanei POLYYYY pio grhgora tis prakseis
    75   36.363 MiB   -0.496 MiB           T = T.todense()
    76   38.617 MiB    2.254 MiB           for i in xrange(n):
    77   38.617 MiB    0.000 MiB               for j in xrange(p):
    78   38.617 MiB    0.000 MiB                   G[i,j+n] = E[i,j] / int(sumQ[i]) # ypologizei ta varh twn query outlinks
    79   38.617 MiB    0.000 MiB               if i in range(n//10,n,n//10): # typwnei to progress
    80                                             print( '%i Percent: %s seconds' %( (i*100//n) + 1, time.time() - time6) )     
    81                                     time7 = time.time()
    82                                     print( 'Total Query Weight Association Time: %s seconds' %(time7 - time6))
    83                                     print( 'URL Weight Association:' )
    84                                     for i in xrange(n):
    85                                         for j in xrange(p):
    86                                             T[i,j] = E[i,j] / sumL.item(j) # ypologizei ta varh twn url outlinks
    87                                         if i in range(n//10,n,n//10): #typwnei to progress
    88                                             print( '%i Percent: %s seconds' %( (i*100//n) + 1, time.time() - time7) ) 
    89                                     time8 = time.time()
    90                                     print( 'Total URL Weight Association Time: %s seconds' %(time8 - time7))
    91                                     del E
    92                                     T = T.transpose() # anastrefei ton pinaka T
    93                                     time9 = time.time()
    94                                     print( 'Transpose Time: %s seconds' %(time9 - time8))
    95                                     print( 'Matrix Join:' )
    96                                     for i in xrange(p):
    97                                         for j in xrange(n):
    98                                             G[i+n,j] = T[i,j] # enwnei tous dyo pinakes
    99                                         if i in range(p//10,p,p//10): # progress
   100                                             print( '%i Percent: %s seconds' %( (i*100//p) + 1, time.time() - time9) )    
   101                                     del T
   102                                     time10 = time.time()
   103                                     print( 'Total Matrix Join Time: %s seconds' %(time10 - time9))
   104                                     #G = lil_matrix(G) # gia na pianei ligotero xwro
   105                                     return G


Traceback (most recent call last):
  File "/usr/lib/python2.7/runpy.py", line 162, in _run_module_as_main
    "__main__", fname, loader, pkg_name)
  File "/usr/lib/python2.7/runpy.py", line 72, in _run_code
    exec code in run_globals
  File "/usr/local/lib/python2.7/dist-packages/memory_profiler.py", line 982, in <module>
    exec_with_profiler(script_filename, prof)
  File "/usr/local/lib/python2.7/dist-packages/memory_profiler.py", line 917, in exec_with_profiler
    execfile(filename, ns, ns)
  File "ConstructSparseGraph.py", line 114, in <module>
    G = constructGraph('AOL15000.csv')
  File "/usr/local/lib/python2.7/dist-packages/memory_profiler.py", line 498, in f
    return func(*args, **kwds)
  File "ConstructSparseGraph.py", line 77, in constructGraph
    for j in xrange(p):
  File "ConstructSparseGraph.py", line 77, in constructGraph
    for j in xrange(p):
  File "/usr/local/lib/python2.7/dist-packages/memory_profiler.py", line 544, in trace_memory_usage
    self.code_map.trace(frame.f_code, self.prevlines[-1])
  File "/usr/local/lib/python2.7/dist-packages/memory_profiler.py", line 440, in trace
    memory = _get_memory(-1, include_children=self.include_children)
  File "/usr/local/lib/python2.7/dist-packages/memory_profiler.py", line 95, in _get_memory
    stdout=subprocess.PIPE
  File "/usr/lib/python2.7/subprocess.py", line 791, in communicate
    stdout = _eintr_retry_call(self.stdout.read)
  File "/usr/lib/python2.7/subprocess.py", line 476, in _eintr_retry_call
    return func(*args)
KeyboardInterrupt
thanos@mars:~/workspace/memory test$ python -m memory_profiler ConstructSparseGraph.py
/usr/local/lib/python2.7/dist-packages/memory_profiler.py:88: UserWarning: psutil module not found. memory_profiler will be slow
  warnings.warn("psutil module not found. memory_profiler will be slow")
Read Time: 114.108469009 seconds
Find Unique Values: 0.550443172455 seconds
Mapping Time: 0.0556879043579 seconds
Construct Accumulative Matrix: 13.0693819523 seconds
Find Node Degrees Time: 0.0406391620636 seconds
Total Time: 127.824621201 seconds
Query Weight Association:
10 Percent: 377.669723988 seconds
20 Percent: 747.067200899 seconds
30 Percent: 1112.79508996 seconds
40 Percent: 1476.5328958 seconds
50 Percent: 1843.3014679 seconds
60 Percent: 2208.20577598 seconds
70 Percent: 2578.27054691 seconds
80 Percent: 2950.92914891 seconds
90 Percent: 3316.40910101 seconds
100 Percent: 3685.40610194 seconds
Total Query Weight Association Time: 3703.02908587 seconds
URL Weight Association:
10 Percent: 377.573843002 seconds
20 Percent: 743.048124075 seconds
30 Percent: 1111.73333001 seconds
40 Percent: 1522.0953691 seconds
50 Percent: 1921.19261193 seconds
60 Percent: 2339.4331851 seconds
70 Percent: 2746.69530296 seconds
80 Percent: 3154.09004307 seconds
90 Percent: 3568.22259307 seconds
100 Percent: 3968.99899602 seconds
Total URL Weight Association Time: 3988.20247602 seconds
Transpose Time: 0.0264339447021 seconds
Matrix Join:
10 Percent: 395.604933977 seconds
20 Percent: 769.66631794 seconds
30 Percent: 1139.40965009 seconds
40 Percent: 1505.10914302 seconds
50 Percent: 1885.03132701 seconds
60 Percent: 2295.43951702 seconds
70 Percent: 2698.53849602 seconds
80 Percent: 3090.18874907 seconds
90 Percent: 3473.95799112 seconds
100 Percent: 3865.09080315 seconds
Total Matrix Join Time: 3871.24672294 seconds
Done!
Matrix Sum : 1115.0
Graph Size : (1115 x 1115)
Graph Memory Size: 144 bytes
--- 11691.5544119 seconds ---
Filename: ConstructSparseGraph.py

Line #    Mem usage    Increment   Line Contents
================================================
    28   25.102 MiB    0.000 MiB   @profile
    29                             def constructGraph(dset):
    30   25.105 MiB    0.004 MiB       time1 = time.time()
    31   25.105 MiB    0.000 MiB       with warnings.catch_warnings(): # giati ta scipy.sparse vgazoun DeprecationWarning
    32   25.105 MiB    0.000 MiB           warnings.simplefilter("ignore")
    33   25.105 MiB    0.000 MiB           with open(dset, 'rb') as tempFile: # diavazei to arxeio
    34   25.105 MiB    0.000 MiB               reader = csv.reader(tempFile)
    35   25.105 MiB    0.000 MiB               q = []
    36   25.105 MiB    0.000 MiB               l = []
    37   25.285 MiB    0.180 MiB               for row in reader:
    38   25.285 MiB    0.000 MiB                   if (row[0] != '') and (row[1] != ''): # xwrizei tis dyo sthles tou arxeiou se dyo listes
    39   25.285 MiB    0.000 MiB                       q.append(row[0])
    40   25.285 MiB    0.000 MiB                       l.append(row[1])
    41   25.281 MiB   -0.004 MiB           time2 = time.time()
    42   25.285 MiB    0.004 MiB           print( 'Read Time: %s seconds' %(time2 - time1) )
    43                                     # Lists with unique values:
    44   25.285 MiB    0.000 MiB           uq=uniqueValues(q) # dhmiourgei listes me monadika stoixeia
    45   25.285 MiB    0.000 MiB           ul=uniqueValues(l)
    46   25.285 MiB    0.000 MiB           time3 = time.time()
    47   25.285 MiB    0.000 MiB           print( 'Find Unique Values: %s seconds' %(time3 - time2) )
    48                                     # n=number of unique queries, p=number of unique url:
    49   25.285 MiB    0.000 MiB           n=len(uq)
    50   25.285 MiB    0.000 MiB           p=len(ul)
    51                                     # Map nodes to queries/url:
    52                                     global Q 
    53   25.434 MiB    0.148 MiB           Q = mapper(uq) # dhmiourgei ordered dictionaries pou antistoixizoun queries me nodes
    54                                     global L
    55   25.586 MiB    0.152 MiB           L = mapper(ul)
    56   25.590 MiB    0.004 MiB           del uq, ul # ypotithetai oti ta eleytherwnei ap th mnhmh, den kserw ti paizei me to garbage collector
    57   25.590 MiB    0.000 MiB           time4 = time.time()
    58   25.590 MiB    0.000 MiB           print( 'Mapping Time: %s seconds' %(time4 - time3) )
    59                                     # Accumulative graph:
    60   25.652 MiB    0.062 MiB           E = lil_matrix((n,p)) # Arxikopoiei ton pinaka E. Ftiaxnoume lil_matrix giati einai pio grhgoroi sthn dhmiourgia.
    61   25.777 MiB    0.125 MiB           for i in xrange(len(q)):
    62   25.777 MiB    0.000 MiB               E[Q.get(q[i])-1,L.get(l[i])-1] +=1 # Dhmioyrgeitai pinakas me antitoixies query-url pou metraei ton arithmo tous. Oi grammes antistoixoun se queries oi sthles se url kai oi times sto plithos twn forwn poy clickare kapoios url me vash to sygkekrimeno query.
    63   25.777 MiB    0.000 MiB           del q, l
    64   25.777 MiB    0.000 MiB           time5 = time.time()
    65   25.777 MiB    0.000 MiB           print( 'Construct Accumulative Matrix: %s seconds' %(time5 - time4) )
    66   25.969 MiB    0.191 MiB           sumQ = np.asarray(E.sum(axis = 1)) # sumQ einai to athroisma twn grammwn. deixnei to degree tou kathe query node
    67   26.285 MiB    0.316 MiB           sumL = np.asarray(E.sum(axis = 0)) # sumL einai to athroisma twn stilwn. deixnei to degree tou kathe url node
    68   26.285 MiB    0.000 MiB           time6 = time.time()
    69   26.285 MiB    0.000 MiB           print( 'Find Node Degrees Time: %s seconds' %(time6 - time5) )
    70   26.285 MiB    0.000 MiB           print( 'Total Time: %s seconds' %(time6 - time1) )
    71   26.344 MiB    0.059 MiB           G = lil_matrix((n+p,n+p)) # Adjacency matrix. Einai tetragwnos pinakas megethous (n+p)x(n+p) opou oi prwtes n grammes/sthles einai ta queries kai oi epomenes p grammes/sthles einai ta url. Logw tou grafou pou exoume o panw aristera ypopinakas nxn kai o katw deksia apo n+1 ews n+p, einai adeioi (afou den syndeontai q me q kai l me l ston grafo mas)
    72   26.402 MiB    0.059 MiB           T = lil_matrix((n,p)) # Einai o katw aristera ypopinakas tou G anastramenos
    73   26.402 MiB    0.000 MiB           print( 'Query Weight Association:' )
    74   26.410 MiB    0.008 MiB           G = G.todense() # kanei POLYYYY pio grhgora tis prakseis
    75   28.410 MiB    2.000 MiB           T = T.todense()
    76   32.172 MiB    3.762 MiB           for i in xrange(n):
    77   32.172 MiB    0.000 MiB               for j in xrange(p):
    78   32.176 MiB    0.004 MiB                   G[i,j+n] = E[i,j] / int(sumQ[i]) # ypologizei ta varh twn query outlinks
    79   32.172 MiB   -0.004 MiB               if i in range(n//10,n,n//10): # typwnei to progress
    80   32.172 MiB    0.000 MiB                   print( '%i Percent: %s seconds' %( (i*100//n) + 1, time.time() - time6) )     
    81   32.172 MiB    0.000 MiB           time7 = time.time()
    82   32.172 MiB    0.000 MiB           print( 'Total Query Weight Association Time: %s seconds' %(time7 - time6))
    83   32.172 MiB    0.000 MiB           print( 'URL Weight Association:' )
    84   32.207 MiB    0.035 MiB           for i in xrange(n):
    85   32.207 MiB    0.000 MiB               for j in xrange(p):
    86   32.207 MiB    0.000 MiB                   T[i,j] = E[i,j] / sumL.item(j) # ypologizei ta varh twn url outlinks
    87   32.207 MiB    0.000 MiB               if i in range(n//10,n,n//10): #typwnei to progress
    88   32.199 MiB   -0.008 MiB                   print( '%i Percent: %s seconds' %( (i*100//n) + 1, time.time() - time7) ) 
    89   32.207 MiB    0.008 MiB           time8 = time.time()
    90   32.207 MiB    0.000 MiB           print( 'Total URL Weight Association Time: %s seconds' %(time8 - time7))
    91   32.207 MiB    0.000 MiB           del E
    92   32.207 MiB    0.000 MiB           T = T.transpose() # anastrefei ton pinaka T
    93   32.207 MiB    0.000 MiB           time9 = time.time()
    94   32.207 MiB    0.000 MiB           print( 'Transpose Time: %s seconds' %(time9 - time8))
    95   32.207 MiB    0.000 MiB           print( 'Matrix Join:' )
    96   37.645 MiB    5.438 MiB           for i in xrange(p):
    97   37.645 MiB    0.000 MiB               for j in xrange(n):
    98   37.645 MiB    0.000 MiB                   G[i+n,j] = T[i,j] # enwnei tous dyo pinakes
    99   37.645 MiB    0.000 MiB               if i in range(p//10,p,p//10): # progress
   100   37.641 MiB   -0.004 MiB                   print( '%i Percent: %s seconds' %( (i*100//p) + 1, time.time() - time9) )    
   101   35.414 MiB   -2.227 MiB           del T
   102   35.414 MiB    0.000 MiB           time10 = time.time()
   103   35.414 MiB    0.000 MiB           print( 'Total Matrix Join Time: %s seconds' %(time10 - time9))
   104                                     #G = lil_matrix(G) # gia na pianei ligotero xwro
   105   35.414 MiB    0.000 MiB           return G

